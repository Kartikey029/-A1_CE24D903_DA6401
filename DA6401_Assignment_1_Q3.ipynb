{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00197141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined loss function\n",
    "def compute_loss(y_true, y_pred, loss_type=\"cross_entropy\"):\n",
    "    if loss_type == \"mse\":\n",
    "        return np.mean(np.square(y_true - y_pred))\n",
    "    elif loss_type == \"cross_entropy\":\n",
    "        return -np.mean(y_true * np.log(y_pred + 1e-8))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid loss function. Choose 'mse' or 'cross_entropy'\")\n",
    "def backpropagation_no_update(X, y, weights, biases, activation_func, loss_type=\"cross_entropy\"):\n",
    "    \"\"\"\n",
    "    Backpropagation that ONLY computes gradients w.r.t. weights and biases,\n",
    "    without performing any parameter updates.\n",
    "    \"\"\"\n",
    "    # Forward pass: obtain activations (h) and pre-activations (a)\n",
    "    h, a = forward_propagation(X, weights, biases, activation_func)\n",
    "    \n",
    "    # Compute the gradient at the output layer.\n",
    "    if loss_type == \"cross_entropy\":\n",
    "        grad_loss = h[-1] - y\n",
    "    elif loss_type == \"mse\":\n",
    "        # For MSE with sigmoid output\n",
    "        grad_loss = (h[-1] - y) * (h[-1] * (1. - h[-1]))\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Loss type '{loss_type}' not implemented.\")\n",
    "    \n",
    "    # Initialize lists to hold gradients for weights and biases.\n",
    "    grad_weights = [None] * len(weights)\n",
    "    grad_biases  = [None] * len(biases)\n",
    "    \n",
    "    # Start backpropagation from the output layer.\n",
    "    grad_h = grad_loss\n",
    "    for i in reversed(range(len(weights))):\n",
    "        \n",
    "        if i != len(weights) - 1:\n",
    "            if activation_func == \"sigmoid\":\n",
    "                grad_a = grad_h * d_sigmoid(a[i])\n",
    "            elif activation_func == \"tanh\":\n",
    "                grad_a = grad_h * d_tanh(a[i])\n",
    "            elif activation_func == \"relu\":\n",
    "                grad_a = grad_h * d_relu(a[i])\n",
    "            elif activation_func == \"identity\":\n",
    "                grad_a = grad_h * d_identity(a[i])  \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation function: {activation_func}\")\n",
    "        else:\n",
    "            grad_a = grad_h  # For the output layer\n",
    "        \n",
    "        # Compute gradients (note: no averaging is performed).\n",
    "        grad_weights[i] = np.dot(h[i].T, grad_a)\n",
    "        grad_biases[i]  = np.sum(grad_a, axis=0, keepdims=True)\n",
    "        \n",
    "        # Propagate the gradient to the previous layer.\n",
    "        grad_h = np.dot(grad_a, weights[i].T)\n",
    "    \n",
    "    return grad_weights, grad_biases\n",
    "        \n",
    "def momentum(x_train, y_train, weights, biases, learning_rate=0.1, momentum_coef=0.5, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Momentum-based optimizer that computes gradients over the entire training set\n",
    "    (i.e. one batch per epoch) and updates the parameters once per epoch, using momentum.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : np.ndarray\n",
    "        Training data (features).\n",
    "    y_train : np.ndarray\n",
    "        Training data (labels).\n",
    "    weights : list of np.ndarray\n",
    "        List of weight matrices for each layer.\n",
    "    biases : list of np.ndarray\n",
    "        List of bias vectors for each layer.\n",
    "    learning_rate : float, optional (default=0.1)\n",
    "        The learning rate for the update.\n",
    "    momentum_coef : float, optional (default=0.5)\n",
    "        The momentum coefficient.\n",
    "    num_epochs : int, optional (default=1)\n",
    "        Number of epochs to run.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : list of np.ndarray\n",
    "        Updated weight matrices.\n",
    "    biases : list of np.ndarray\n",
    "        Updated bias vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize momentum vectors for weights and biases (velocity)\n",
    "    velocity_w = [np.zeros_like(w) for w in weights]\n",
    "    velocity_b = [np.zeros_like(b) for b in biases]\n",
    "    y_train = y_train.flatten()\n",
    "    batch_size = 64 \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            batch_x = x_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            h, _ = forward_propagation(batch_x, weights, biases, config[\"activation\"])\n",
    "\n",
    "            # Compute loss using matching batch sizes\n",
    "            loss = compute_loss(batch_y, h[-1], config[\"loss\"])\n",
    "\n",
    "            # Calculate gradients based on this batch\n",
    "            grad_w, grad_b = backpropagation_no_update(\n",
    "                batch_x, batch_y,\n",
    "                weights.copy(), biases.copy(),\n",
    "                activation_func=config[\"activation\"],\n",
    "                loss_type=config[\"loss\"]\n",
    "                 )\n",
    "            # Update parameters using momentum\n",
    "            for j in range(len(weights)):\n",
    "                # Update velocity: v = momentum_coef * v + learning_rate * accumulated_gradient\n",
    "                velocity_w[j] = momentum_coef * velocity_w[j] + learning_rate * grad_w[j]\n",
    "                velocity_b[j] = momentum_coef * velocity_b[j] + learning_rate * grad_b[j]\n",
    "\n",
    "                # Update parameters (SGD update with momentum)\n",
    "                weights[j] -= velocity_w[j]\n",
    "                biases[j]  -= velocity_b[j]\n",
    "\n",
    "      \n",
    "    return weights, biases\n",
    "def nag(x_train, y_train, weights, biases, learning_rate=0.1, momentum_coef=0.5, num_epochs=1, loss_type=\"cross_entropy\"):\n",
    "    # Initialize velocity vectors for weights and biases (momentum)\n",
    "    velocity_w = [np.zeros_like(w) for w in weights]\n",
    "    velocity_b = [np.zeros_like(b) for b in biases]\n",
    "    y_train = y_train.flatten()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Use the entire training set as one batch\n",
    "        batch_x = x_train\n",
    "        batch_y = np.eye(10)[y_train.astype(int)]\n",
    "\n",
    "#         batch_y = np.eye(10)[y_train[start:end].astype(int)]\n",
    "\n",
    "        # Lookahead step: temporarily adjust parameters using momentum\n",
    "        temp_weights = [weights[j] - momentum_coef * velocity_w[j] for j in range(len(weights))]\n",
    "        temp_biases  = [biases[j] - momentum_coef * velocity_b[j] for j in range(len(biases))]\n",
    "        \n",
    "        # Forward pass using lookahead parameters\n",
    "        h, _ = forward_propagation(batch_x, temp_weights, temp_biases, config[\"activation\"])\n",
    "        total_loss = compute_loss(batch_y, h[-1], loss_type)\n",
    "        \n",
    "        # Compute gradients using lookahead parameters\n",
    "        grad_w, grad_b = backpropagation_no_update(\n",
    "            batch_x, batch_y,\n",
    "            temp_weights.copy(), temp_biases.copy(),\n",
    "           activation_func=config[\"activation\"],\n",
    "            loss_type=config[\"loss\"]\n",
    "             )\n",
    "        \n",
    "        # Update velocities and parameters using NAG update rule\n",
    "        for j in range(len(weights)):\n",
    "            velocity_w[j] = momentum_coef * velocity_w[j] + learning_rate * grad_w[j]\n",
    "            velocity_b[j] = momentum_coef * velocity_b[j] + learning_rate * grad_b[j]\n",
    "            \n",
    "            weights[j] -= velocity_w[j]\n",
    "            biases[j]  -= velocity_b[j]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set (assuming x_test and y_test are defined globally)\n",
    "    h_test, _ = forward_propagation(x_test, weights, biases, config[\"activation\"])\n",
    "    y_pred_test = np.argmax(h_test[-1], axis=1)\n",
    "    test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "# def nag(x_train, y_train, weights, biases, learning_rate=0.1, momentum_coef=0.5, num_epochs=1):\n",
    "#     \"\"\"\n",
    "#     Nesterov Accelerated Gradient (NAG) optimizer that uses the entire training set as a single batch per epoch.\n",
    "#     It applies a lookahead step before computing the gradients and updates parameters once per epoch.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     x_train : np.ndarray\n",
    "#         Training data (features).\n",
    "#     y_train : np.ndarray\n",
    "#         Training data (labels).\n",
    "#     weights : list of np.ndarray\n",
    "#         List of weight matrices for each layer.\n",
    "#     biases : list of np.ndarray\n",
    "#         List of bias vectors for each layer.\n",
    "#     learning_rate : float, optional (default=0.1)\n",
    "#         The learning rate for the update.\n",
    "#     momentum_coef : float, optional (default=0.5)\n",
    "#         The momentum coefficient.\n",
    "#     num_epochs : int, optional (default=1)\n",
    "#         Number of epochs to run.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     test_accuracy : float\n",
    "#         The test accuracy computed after training (assumes x_test and y_test are defined globally).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Initialize velocity vectors for weights and biases (momentum)\n",
    "#     velocity_w = [np.zeros_like(w) for w in weights]\n",
    "#     velocity_b = [np.zeros_like(b) for b in biases]\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Use the entire training set as one batch\n",
    "#         batch_x = x_train\n",
    "# #         batch_y = np.eye(10)[y_train]  # One-hot encoding\n",
    "#         batch_y = np.eye(10)[y_train.astype(int)]  # One-hot encoding with integer indices\n",
    "     \n",
    "#         # Lookahead step: temporarily adjust parameters using momentum\n",
    "#         temp_weights = [weights[j] - momentum_coef * velocity_w[j] for j in range(len(weights))]\n",
    "#         temp_biases  = [biases[j] - momentum_coef * velocity_b[j] for j in range(len(biases))]\n",
    "        \n",
    "#         # Forward pass using lookahead parameters\n",
    "#         h, _ = forward_propagation(batch_x, temp_weights, temp_biases)\n",
    "#         total_loss = compute_loss(batch_y, h[-1], config.loss)\n",
    "        \n",
    "#         # Compute gradients using lookahead parameters\n",
    "#         grad_w, grad_b = backpropagation_no_update(\n",
    "#             batch_x, batch_y,\n",
    "#             temp_weights.copy(), temp_biases.copy(),\n",
    "#             activation_func=config.activation,\n",
    "#             loss_type=config.loss\n",
    "#         )\n",
    "        \n",
    "#         # Update velocities and parameters using NAG update rule\n",
    "#         for j in range(len(weights)):\n",
    "#             # Update velocity: v = momentum_coef * v + learning_rate * grad\n",
    "#             velocity_w[j] = momentum_coef * velocity_w[j] + learning_rate * grad_w[j]\n",
    "#             velocity_b[j] = momentum_coef * velocity_b[j] + learning_rate * grad_b[j]\n",
    "            \n",
    "#             # Update parameters: parameter = parameter - velocity\n",
    "#             weights[j] -= velocity_w[j]\n",
    "#             biases[j]  -= velocity_b[j]\n",
    "        \n",
    "#         wandb.log({\"epoch\": epoch + 1, \"loss\": total_loss})\n",
    "    \n",
    "#     # Evaluate test accuracy (assumes x_test and y_test are defined globally)\n",
    "#     h_test, _ = forward_propagation(x_test, weights.copy(), biases.copy(), activation_func=config.activation)\n",
    "#     y_pred_test = np.argmax(h_test[-1], axis=1)\n",
    "#     test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "#     wandb.log({\"test_accuracy\": test_accuracy})\n",
    "    \n",
    "#     return test_accuracy\n",
    "\n",
    "def sgd(x_train, y_train, weights, biases, learning_rate=0.1, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Pure Stochastic Gradient Descent (SGD) optimizer that updates parameters\n",
    "    after processing each individual training sample.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : np.ndarray\n",
    "        Training data (features), shape (num_samples, num_features).\n",
    "    y_train : np.ndarray\n",
    "        Training labels (integers), shape (num_samples,).\n",
    "    weights : list of np.ndarray\n",
    "        List of weight matrices for each layer.\n",
    "    biases : list of np.ndarray\n",
    "        List of bias vectors for each layer.\n",
    "    learning_rate : float, optional (default=0.1)\n",
    "        The learning rate for the parameter updates.\n",
    "    num_epochs : int, optional (default=1)\n",
    "        Number of epochs to run.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    test_accuracy : float\n",
    "        The accuracy on the test set (assuming x_test, y_test, etc. are defined globally).\n",
    "    \"\"\"\n",
    "    num_samples = len(x_train)\n",
    "    y_train = y_train.flatten()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Optionally, shuffle the training data at the beginning of each epoch\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = x_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        y_train = y_train.flatten()\n",
    "\n",
    "        # Process each training sample one-by-one\n",
    "        for i in range(num_samples):\n",
    "            # Extract a single sample and reshape for forward propagation\n",
    "            sample_x = x_train[i].reshape(1, -1)  # shape: (1, num_features)\n",
    "            # Convert the scalar label to a one-hot vector\n",
    "            sample_y = np.eye(10)[int(y_train[i].item())].reshape(1, -1)\n",
    "\n",
    "\n",
    "\n",
    "            # Forward pass on the single sample\n",
    "            h, _ = forward_propagation(sample_x, weights, biases, config[\"activation\"])\n",
    "            loss = compute_loss(sample_y, h[-1], config[\"loss\"])\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients for the single sample\n",
    "            grad_w, grad_b = backpropagation_no_update(\n",
    "                sample_x, sample_y,\n",
    "                weights.copy(), biases.copy(),\n",
    "                activation_func=config[\"activation\"],\n",
    "                loss_type=config[\"loss\"] )\n",
    "                 \n",
    "\n",
    "            # Update parameters immediately (pure SGD)\n",
    "            for j in range(len(weights)):\n",
    "                weights[j] -= learning_rate * grad_w[j]\n",
    "                biases[j]  -= learning_rate * grad_b[j]\n",
    "        \n",
    "        wandb.log({\"epoch\": epoch + 1, \"loss\": total_loss})\n",
    "    \n",
    "    # Evaluate on test data (assuming x_test and y_test are defined globally)\n",
    "    h_test, _ = forward_propagation(x_test, weights, biases, config[\"activation\"])\n",
    "    y_pred_test = np.argmax(h_test[-1], axis=1)\n",
    "    test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "    \n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "def rmsprop(x_train, y_train, weights, biases, \n",
    "                 learning_rate=0.1, beta=0.5, eps=0.000001, num_epochs=1):\n",
    "    \"\"\"\n",
    "    RMSProp optimizer in a pure stochastic setting, updating parameters after each sample.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : np.ndarray\n",
    "        Training data (features), shape (num_samples, num_features).\n",
    "    y_train : np.ndarray\n",
    "        Training labels (integers), shape (num_samples,).\n",
    "    weights : list of np.ndarray\n",
    "        List of weight matrices for each layer.\n",
    "    biases : list of np.ndarray\n",
    "        List of bias vectors for each layer.\n",
    "    learning_rate : float, optional (default=0.1)\n",
    "        The learning rate for parameter updates.\n",
    "    beta : float, optional (default=0.5)\n",
    "        Decay rate for the moving average of squared gradients.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small constant to avoid division by zero.\n",
    "    num_epochs : int, optional (default=1)\n",
    "        Number of epochs to run.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    test_accuracy : float\n",
    "        The accuracy on the test set (assuming x_test, y_test, etc. are defined globally).\n",
    "    \"\"\"\n",
    "    num_samples = len(x_train)\n",
    "\n",
    "    # Initialize RMSProp accumulators for squared gradients\n",
    "    v_w = [np.zeros_like(w) for w in weights]\n",
    "    v_b = [np.zeros_like(b) for b in biases]\n",
    "    y_train = y_train.flatten()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Shuffle the training data at the beginning of each epoch (optional, but common in SGD)\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = x_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # Process each training sample one-by-one\n",
    "        for i in range(num_samples):\n",
    "            # Extract a single sample and reshape for forward propagation\n",
    "            sample_x = x_train[i].reshape(1, -1)  # shape: (1, num_features)\n",
    "            # Convert the scalar label to a one-hot vector\n",
    "            sample_y = np.eye(10)[int(y_train[i].item())].reshape(1, -1)\n",
    "\n",
    "            # Forward pass on the single sample\n",
    "            h, _ = forward_propagation(sample_x, weights, biases, config[\"activation\"])\n",
    "            loss = compute_loss(sample_y, h[-1], config[\"loss\"])\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients for the single sample\n",
    "            grad_w, grad_b = backpropagation_no_update(\n",
    "                sample_x, sample_y,\n",
    "                weights.copy(), biases.copy(),\n",
    "                activation_func=config[\"activation\"],\n",
    "                loss_type=config[\"loss\"]\n",
    "                 )\n",
    "\n",
    "            # === RMSProp Update for Each Layer ===\n",
    "            for j in range(len(weights)):\n",
    "                # Update the moving average of the squared gradients\n",
    "                v_w[j] = beta * v_w[j] + (1 - beta) * (grad_w[j] ** 2)\n",
    "                v_b[j] = beta * v_b[j] + (1 - beta) * (grad_b[j] ** 2)\n",
    "\n",
    "                # Update parameters: W -= lr * grad / (sqrt(v) + eps)\n",
    "                weights[j] -= learning_rate * grad_w[j] / (np.sqrt(v_w[j]) + eps)\n",
    "                biases[j]  -= learning_rate * grad_b[j] / (np.sqrt(v_b[j]) + eps)\n",
    "        \n",
    "       \n",
    "    \n",
    "    # Evaluate on test data (assuming x_test and y_test are defined globally)\n",
    "    h_test, _ = forward_propagation(x_test, weights, biases, config[\"activation\"])\n",
    "    y_pred_test = np.argmax(h_test[-1], axis=1)\n",
    "    test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "    \n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "def adam(x_train, y_train, weights, biases,learning_rate=0.1, beta1=0.5, beta2=0.5, eps=0.000001, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Adam optimizer in a pure stochastic setting, updating parameters after each sample.\n",
    "    (Simplified: no bias correction for the first and second moments.)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : np.ndarray\n",
    "        Training data (features), shape (num_samples, num_features).\n",
    "    y_train : np.ndarray\n",
    "        Training labels (integers), shape (num_samples,).\n",
    "    weights : list of np.ndarray\n",
    "        List of weight matrices for each layer.\n",
    "    biases : list of np.ndarray\n",
    "        List of bias vectors for each layer.\n",
    "    learning_rate : float, optional (default=0.1)\n",
    "        The learning rate for parameter updates.\n",
    "    beta1 : float, optional (default=0.5)\n",
    "        Exponential decay rate for the first moment (moving average of gradients).\n",
    "    beta2 : float, optional (default=0.5)\n",
    "        Exponential decay rate for the second moment (moving average of squared gradients).\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small constant to avoid division by zero.\n",
    "    num_epochs : int, optional (default=1)\n",
    "        Number of epochs to run.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    test_accuracy : float\n",
    "        The accuracy on the test set (assuming x_test, y_test, etc. are defined globally).\n",
    "    \"\"\"\n",
    "    num_samples = len(x_train)\n",
    "\n",
    "    # Initialize first (m) and second (v) moments for each layer\n",
    "    m_w = [np.zeros_like(w) for w in weights]\n",
    "    m_b = [np.zeros_like(b) for b in biases]\n",
    "    v_w = [np.zeros_like(w) for w in weights]\n",
    "    v_b = [np.zeros_like(b) for b in biases]\n",
    "    y_train = y_train.flatten()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Shuffle the training data at the beginning of each epoch (optional)\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = x_train[indices]\n",
    "\n",
    "        # Process each training sample individually\n",
    "        for i in range(num_samples):\n",
    "            # Extract a single sample\n",
    "            sample_x = x_train[i].reshape(1, -1)  # shape: (1, num_features)\n",
    "            sample_y = np.eye(10)[int(y_train[i].item())].reshape(1, -1)\n",
    "\n",
    "            # Forward pass on the single sample\n",
    "            h, _ = forward_propagation(sample_x, weights, biases, config[\"activation\"])\n",
    "            loss = compute_loss(sample_y, h[-1], config[\"loss\"])\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients for the single sample\n",
    "            grad_w, grad_b = backpropagation_no_update(\n",
    "                sample_x, sample_y,\n",
    "                weights.copy(), biases.copy(),\n",
    "                activation_func=config[\"activation\"],\n",
    "                loss_type=config[\"loss\"]\n",
    "                 )\n",
    "\n",
    "            # === Adam Update for Each Layer (no bias correction) ===\n",
    "            for j in range(len(weights)):\n",
    "                # Update first moment\n",
    "                m_w[j] = beta1 * m_w[j] + (1 - beta1) * grad_w[j]\n",
    "                m_b[j] = beta1 * m_b[j] + (1 - beta1) * grad_b[j]\n",
    "\n",
    "                # Update second moment\n",
    "                v_w[j] = beta2 * v_w[j] + (1 - beta2) * (grad_w[j] ** 2)\n",
    "                v_b[j] = beta2 * v_b[j] + (1 - beta2) * (grad_b[j] ** 2)\n",
    "\n",
    "                # Parameter update: param -= lr * (m / sqrt(v) + eps)\n",
    "                weights[j] -= learning_rate * (m_w[j] / (np.sqrt(v_w[j]) + eps))\n",
    "                biases[j]  -= learning_rate * (m_b[j] / (np.sqrt(v_b[j]) + eps))\n",
    "\n",
    "        # Log the total loss for the epoch\n",
    "\n",
    "# h, _ = forward_propagation(batch_x, weights, biases, config[\"activation\"])\n",
    "\n",
    "    # Evaluate on test data (assuming x_test and y_test are defined globally)\n",
    "    h_test, _ = forward_propagation(x_test, weights, biases, config[\"activation\"])\n",
    "    y_pred_test = np.argmax(h_test[-1], axis=1)\n",
    "    test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "    \n",
    "\n",
    "    return test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
