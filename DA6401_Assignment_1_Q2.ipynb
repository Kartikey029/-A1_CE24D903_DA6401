{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922bf078",
   "metadata": {},
   "source": [
    "Question 2 \n",
    "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b19400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Random Weights): 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "def load_dataset(dataset=\"fashion_mnist\"):\n",
    "    from keras.datasets import mnist, fashion_mnist\n",
    "    if dataset == \"fashion_mnist\":\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    elif dataset == \"mnist\":\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset choice.\")\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "    \n",
    "    # Flatten the label arrays so each label is a scalar\n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset()\n",
    "y_train = y_train.flatten()\n",
    "\n",
    "# Reshape and normalize data\n",
    "x_train = x_train.reshape(x_train.shape[0], 784) / 256\n",
    "x_test = x_test.reshape(x_test.shape[0], 784) / 256\n",
    "\n",
    "def get_user_defined_layers(num_hidden_layers=1, neurons_per_layer=[4]):\n",
    "    # If neurons_per_layer is an int, convert it to a list of that int repeated num_hidden_layers times.\n",
    "    if isinstance(neurons_per_layer, int):\n",
    "        neurons_per_layer = [neurons_per_layer] * num_hidden_layers\n",
    "    \n",
    "    input_size = 784\n",
    "    output_size = 10\n",
    "\n",
    "    if len(neurons_per_layer) != num_hidden_layers:\n",
    "        raise ValueError(f\"Number of hidden layers ({num_hidden_layers}) does not match the number of neuron lists provided ({len(neurons_per_layer)}).\")\n",
    "    \n",
    "    return [input_size] + neurons_per_layer + [output_size]\n",
    "\n",
    "# # Function to take user-defined network architecture\n",
    "# def get_user_defined_layers(num_hidden_layers=1, neurons_per_layer=[4]):\n",
    "#     input_size = 784\n",
    "#     output_size = 10\n",
    "\n",
    "#     # Check if the number of hidden layers matches the number of neuron lists provided\n",
    "#     if len(neurons_per_layer) != num_hidden_layers:\n",
    "#         raise ValueError(f\"Number of hidden layers ({num_hidden_layers}) does not match the number of neuron lists provided ({len(neurons_per_layer)}).\")\n",
    "\n",
    "#     return [input_size] + neurons_per_layer + [output_size]\n",
    "\n",
    "# Get the architecture from the user\n",
    "layer_sizes = get_user_defined_layers(num_hidden_layers=3, neurons_per_layer=[4, 5, 6])\n",
    "\n",
    "# Weight Initialization Methods\n",
    "def random_init(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * 0.01\n",
    "\n",
    "def xavier_init(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(2 / (input_size + output_size))\n",
    "\n",
    "def initialize_weights_and_biases(layer_sizes, init_method=\"random\"):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - layer_sizes: List of integers representing the sizes of each layer.\n",
    "    - init_method: String indicating the initialization method. Defaults to \"random\". Can be \"random\" or \"xavier\".\n",
    "\n",
    "    Returns:\n",
    "    - weights: List of initialized weight matrices.\n",
    "    - biases: List of initialized bias vectors.\n",
    "    \"\"\"\n",
    "    if init_method not in [\"random\", \"xavier\"]:\n",
    "        raise ValueError(f\"Invalid initialization method: {init_method}\")\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        if init_method == \"random\":\n",
    "            W = random_init(layer_sizes[i], layer_sizes[i + 1])\n",
    "        elif init_method == \"xavier\":\n",
    "            W = xavier_init(layer_sizes[i], layer_sizes[i + 1])\n",
    "        \n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# Initialize weights and biases with default random initialization\n",
    "weights, biases = initialize_weights_and_biases(layer_sizes)\n",
    "\n",
    "# Activation Functions\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def identity(Z):\n",
    "    return Z\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Derivatives of Activation Functions\n",
    "def d_identity(x):\n",
    "    return 1\n",
    "\n",
    "def d_relu(x):\n",
    "    return (x > 0).astype(int)  # Vectorized ReLU derivative\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 - np.square(np.tanh(x))\n",
    "\n",
    "# Softmax Function\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z)\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "# Dictionary to map function names to implementations\n",
    "activation_functions = {\n",
    "    \"identity\": identity,\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"tanh\": tanh,\n",
    "    \"relu\": relu\n",
    "}\n",
    "activation_func = activation_functions\n",
    "# Forward Propagation with selectable activation function\n",
    "def forward_propagation(X, weights, biases, activation=\"sigmoid\"):\n",
    "    if activation not in activation_functions:\n",
    "        raise ValueError(f\"Invalid activation function: {activation}\")\n",
    "\n",
    "    activation_func = activation_functions[activation]\n",
    "\n",
    "    h = [X]  # Store all activation outputs\n",
    "    a = []   # Store all weighted sums\n",
    "\n",
    "    for i in range(len(weights) - 1):  \n",
    "        a_i = np.dot(h[-1], weights[i]) + biases[i]\n",
    "        a.append(a_i)\n",
    "        h_i = activation_func(a_i)  \n",
    "        h.append(h_i)\n",
    "\n",
    "    # Output Layer (always using softmax)\n",
    "    a_final = np.dot(h[-1], weights[-1]) + biases[-1]\n",
    "    a.append(a_final)\n",
    "    h_final = softmax(a_final)  \n",
    "    h.append(h_final)\n",
    "\n",
    "    return h, a  \n",
    "# Forward Propagation for Training Data\n",
    "yhat, a_values = forward_propagation(x_train, weights, biases)\n",
    "\n",
    "# Prediction\n",
    "def predict(X, weights, biases):\n",
    "    _, _ = forward_propagation(X, weights, biases)\n",
    "    # Since forward_propagation returns lists, we need to get the last element of h (which is the output)\n",
    "    # However, the last element of h is already the softmax output, so we directly use it\n",
    "    h_values, _ = forward_propagation(X, weights, biases)\n",
    "    return np.argmax(h_values[-1], axis=1)\n",
    "\n",
    "# Accuracy Calculation\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100  \n",
    "\n",
    "# Apply Model on Test Data (Random Weights)\n",
    "y_pred = predict(x_test, weights, biases)\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy = compute_accuracy(y_test, y_pred)\n",
    "print(f\"Test Accuracy (Random Weights): {accuracy:.2f}%\")\n",
    "\n",
    "# Forward Propagation for Training Data\n",
    "h_values, a_values = forward_propagation(x_train, weights, biases, activation=\"relu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
